{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7ab20f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the DataFrame:\n",
      "                             tags\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# QUESTION-1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "required = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "search = BeautifulSoup(required.text, 'html.parser')\n",
    "\n",
    "\n",
    "tags = [header.text.strip() for header in search.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "\n",
    "df = pd.DataFrame({'tags': tags})\n",
    "\n",
    "print(\"Displaying the DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f91a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Former Presidents of India:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# QUESTION-2 (Provided link not working but following is the relevant code)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "presidents_info = []\n",
    "for president_div in soup.find_all('div', class_='col-md-4'):\n",
    "    name_elem = president_div.find('h4')\n",
    "    if name_elem:\n",
    "        name = name_elem.text.strip()\n",
    "        term_of_office_elem = president_div.find('p')\n",
    "        term_of_office = term_of_office_elem.text.strip().split(\":\")[1].strip() if term_of_office_elem else \"N/A\"\n",
    "        presidents_info.append({'Name': name, 'Term of Office': term_of_office}\n",
    "df = pd.DataFrame(presidents_info)\n",
    "print(\"List of Former Presidents of India:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f774090d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# QUESTION-3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape and create DataFrame for Top 10 ODI Teams\n",
    "url_teams = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "required = requests.get(url_teams)\n",
    "soup_teams = BeautifulSoup(required.text, 'html.parser')\n",
    "\n",
    "teams_info = []\n",
    "for team_div in soup_teams.find_all('tr', class_='table-body__row')[:10]:\n",
    "    team_name = team_div.find('span', class_='u-hide-phablet').text.strip()\n",
    "    \n",
    "    matches_tag = team_div.find('td', class_='table-body__cell u-center-text')\n",
    "    matches = matches_tag.text.strip() if matches_tag else ''\n",
    "    \n",
    "    points_tag = team_div.find('td', class_='table-body__cell u-center-text')\n",
    "    points = points_tag.text.strip() if points_tag else ''\n",
    "    \n",
    "    rating = team_div.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "    \n",
    "    teams_info.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "df_teams = pd.DataFrame(teams_info)\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(df_teams)\n",
    "\n",
    "\n",
    "url_batsmen = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "required_batsmen = requests.get(url_batsmen)\n",
    "soup_batsmen = BeautifulSoup(required_batsmen.text, 'html.parser')\n",
    "\n",
    "batsmen_info = []\n",
    "for player_div in soup_batsmen.find_all('tr', class_='table-body__row')[:10]:\n",
    "    player_name = player_div.find('td', class_='table-body__cell name').text.strip()\n",
    "    team = player_div.find('span', class_='table-body__logo-text').text.strip()\n",
    "    rating = player_div.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "    \n",
    "    batsmen_info.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "df_batsmen = pd.DataFrame(batsmen_info)\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(df_batsmen)\n",
    "\n",
    "# Scrape and create DataFrame for Top 10 ODI Bowlers\n",
    "url_bowlers = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "required_bowlers = requests.get(url_bowlers)\n",
    "soup_bowlers = BeautifulSoup(required_bowlers.text, 'html.parser')\n",
    "\n",
    "bowlers_info = []\n",
    "for player_div in soup_bowlers.find_all('tr', class_='table-body__row')[:10]:\n",
    "    player_name = player_div.find('td', class_='table-body__cell name').text.strip()\n",
    "    team = player_div.find('span', class_='table-body__logo-text').text.strip()\n",
    "    rating = player_div.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "    \n",
    "    bowlers_info.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "df_bowlers = pd.DataFrame(bowlers_info)\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(df_bowlers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f214f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Top 10 Women's ODI Batting Players:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Top 10 Women's ODI All-Rounders:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# QUESTION-4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape and create DataFrame for Top 10 Women's ODI Teams\n",
    "url_teams = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response_teams = requests.get(url_teams)\n",
    "soup_teams = BeautifulSoup(response_teams.text, 'html.parser')\n",
    "\n",
    "teams_info = []\n",
    "for team_div in soup_teams.find_all('tr', class_='table-body__row')[:10]:\n",
    "    team_name = team_div.find('span', class_='u-hide-phablet').text.strip()\n",
    "    \n",
    "    matches_tag = team_div.find('td', class_='table-body__cell u-center-text')\n",
    "    matches = matches_tag.text.strip() if matches_tag else ''\n",
    "    \n",
    "    points_tag = team_div.find('td', class_='table-body__cell u-center-text')\n",
    "    points = points_tag.text.strip() if points_tag else ''\n",
    "    \n",
    "    rating = team_div.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "    \n",
    "    teams_info.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "df_teams = pd.DataFrame(teams_info)\n",
    "print(\"Top 10 Women's ODI Teams:\")\n",
    "print(df_teams)\n",
    "\n",
    "# Scrape and create DataFrame for Top 10 Women's ODI Batting Players\n",
    "url_batting = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response_batting = requests.get(url_batting)\n",
    "soup_batting = BeautifulSoup(response_batting.text, 'html.parser')\n",
    "\n",
    "batting_info = []\n",
    "for player_div in soup_batting.find_all('tr', class_='table-body__row')[:10]:\n",
    "    player_name = player_div.find('td', class_='table-body__cell name').text.strip()\n",
    "    team = player_div.find('span', class_='table-body__logo-text').text.strip()\n",
    "    rating = player_div.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "    \n",
    "    batting_info.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "df_batting = pd.DataFrame(batting_info)\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "print(df_batting)\n",
    "\n",
    "# Scrape and create DataFrame for Top 10 Women's ODI All-Rounders\n",
    "url_allrounders = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "response_allrounders = requests.get(url_allrounders)\n",
    "soup_allrounders = BeautifulSoup(response_allrounders.text, 'html.parser')\n",
    "\n",
    "allrounders_info = []\n",
    "for player_div in soup_allrounders.find_all('tr', class_='table-body__row')[:10]:\n",
    "    player_name = player_div.find('td', class_='table-body__cell name').text.strip()\n",
    "    team = player_div.find('span', class_='table-body__logo-text').text.strip()\n",
    "    rating = player_div.find('td', class_='table-body__cell u-text-right rating').text.strip()\n",
    "    \n",
    "    allrounders_info.append({'Player': player_name, 'Team': team, 'Rating': rating})\n",
    "\n",
    "df_allrounders = pd.DataFrame(allrounders_info)\n",
    "print(\"\\nTop 10 Women's ODI All-Rounders:\")\n",
    "print(df_allrounders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d094748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Details from CNBC:\n",
      "                                             Headline Time  \\\n",
      "0   Stocks making the biggest moves midday: Tesla,...        \n",
      "1   Dow jumps 250 points, building on 2023 gains a...        \n",
      "2   Fed Chair Powell calls talk of cutting rates '...        \n",
      "3   Treasury yields fall even as Powell says rate-...        \n",
      "4   Bill Gates warns the world is likely to smash ...        \n",
      "5   UAE commits $30 billion to new climate-focused...        \n",
      "6   Progress on sustainability in the fashion indu...        \n",
      "7   Estonian PM calls on global powers to emulate ...        \n",
      "8   Bill Gates shares his ‘big hope’ for the COP28...        \n",
      "9   Op-ed: With continued geopolitical conflicts, ...        \n",
      "10  Zelenskyy says 'new phase of war' has begun, H...        \n",
      "11  Multiple civilians injured in strikes on Ukrai...        \n",
      "12  Russia slams Finland's border closure, warns t...        \n",
      "13  Finland to close entire border with Russia; wi...        \n",
      "14               CNBC's Sustainable Future Forum 2023        \n",
      "15  Wind power industry in moment of reckoning as ...        \n",
      "16  CEO on why natural gas infrastructure must be ...        \n",
      "17  Why one Tesla manager thinks used cars are 'ab...        \n",
      "18  Volvo Cars CEO strikes cautious tone on solid-...        \n",
      "19  Southeast Asia is on the cusp of a 'supercharg...        \n",
      "20  Cambodia's deputy prime minister: BRI has help...        \n",
      "21  Southeast Asia's first luxury hotel made from ...        \n",
      "22  Ahead of Indonesia’s elections, critics slam J...        \n",
      "23  Laos is spiraling toward a debt crisis as Chin...        \n",
      "24  Pack your jerseys. The era of 'sports tourism'...        \n",
      "25  See the photos that made National Geographic's...        \n",
      "26  The ultimate work perk? This company provides ...        \n",
      "27  Fear is driving Chinese travelers away from tw...        \n",
      "28  A UNESCO World Heritage site with thousands of...        \n",
      "29  The No. 1 'desirable' trait CEOs look for in e...        \n",
      "30  Your network is the key to your career, says T...        \n",
      "31  Parenting tips from a dad who’s fostered 36 ki...        \n",
      "32  6 tips to really disconnect from work for the ...        \n",
      "33  Study: Anxious dads raise smarter, more well-b...        \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/12/01/stocks-making-...  \n",
      "1   https://www.cnbc.com/2023/11/30/stock-market-t...  \n",
      "2   https://www.cnbc.com/2023/12/01/fed-chair-powe...  \n",
      "3   https://www.cnbc.com/2023/12/01/us-treasury-yi...  \n",
      "4   https://www.cnbc.com/2023/12/01/bill-gates-war...  \n",
      "5   https://www.cnbc.com/2023/12/01/uae-commits-30...  \n",
      "6   https://www.cnbc.com/video/2023/12/01/sustaina...  \n",
      "7   https://www.cnbc.com/video/2023/12/01/estonian...  \n",
      "8   https://www.cnbc.com/2023/12/01/bill-gates-sha...  \n",
      "9   https://www.cnbc.com/2023/12/01/op-ed-war-is-a...  \n",
      "10  https://www.cnbc.com/2023/12/01/russia-ukraine...  \n",
      "11  https://www.cnbc.com/2023/11/30/ukraine-war-li...  \n",
      "12  https://www.cnbc.com/2023/11/29/ukraine-war-li...  \n",
      "13  https://www.cnbc.com/2023/11/28/ukraine-war-li...  \n",
      "14  https://www.cnbc.com/2023/11/29/cnbcs-sustaina...  \n",
      "15  https://www.cnbc.com/2023/11/13/wind-power-ind...  \n",
      "16  https://www.cnbc.com/2023/11/09/ceo-on-why-nat...  \n",
      "17  https://www.cnbc.com/2023/11/02/one-tesla-mana...  \n",
      "18  https://www.cnbc.com/2023/10/27/volvo-cars-ceo...  \n",
      "19  https://www.cnbc.com/2023/11/29/southeast-asia...  \n",
      "20  https://www.cnbc.com/video/2023/11/27/cambodia...  \n",
      "21  https://www.cnbc.com/2023/11/27/a-new-luxury-h...  \n",
      "22  https://www.cnbc.com/2023/11/20/indonesia-pres...  \n",
      "23  https://www.cnbc.com/2023/11/09/laos-is-spiral...  \n",
      "24  https://www.cnbc.com/2023/11/27/pack-your-jers...  \n",
      "25  https://www.cnbc.com/2023/11/23/best-photograp...  \n",
      "26  https://www.cnbc.com/2023/11/23/remote-company...  \n",
      "27  https://www.cnbc.com/2023/11/20/fear-driving-c...  \n",
      "28  https://www.cnbc.com/2023/11/12/a-unesco-world...  \n",
      "29  https://www.cnbc.com/2023/12/01/the-no-1-desir...  \n",
      "30  https://www.cnbc.com/video/2023/12/01/your-net...  \n",
      "31  https://www.cnbc.com/2023/12/01/parenting-tips...  \n",
      "32  https://www.cnbc.com/2023/11/30/6-tips-to-real...  \n",
      "33  https://www.cnbc.com/2023/11/30/study-anxious-...  \n"
     ]
    }
   ],
   "source": [
    "# QUESTION-5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape and create DataFrame for news details from CNBC\n",
    "url_news = 'https://www.cnbc.com/world/?region=world'\n",
    "response_news = requests.get(url_news)\n",
    "soup_news = BeautifulSoup(response_news.text, 'html.parser')\n",
    "\n",
    "news_info = []\n",
    "for news_div in soup_news.find_all('div', class_='Card-titleContainer'):\n",
    "    headline = news_div.find('a').text.strip()\n",
    "    \n",
    "    time_tag = news_div.find('time')\n",
    "    time = time_tag.text.strip() if time_tag else ''\n",
    "    \n",
    "    news_link = news_div.find('a')['href']\n",
    "    \n",
    "    news_info.append({'Headline': headline, 'Time': time, 'News Link': news_link})\n",
    "\n",
    "df_news = pd.DataFrame(news_info)\n",
    "print(\"News Details from CNBC:\")\n",
    "print(df_news)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3b18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Downloaded Articles in AI (Last 90 days):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# QUESTION-6\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "required = requests.get(url)\n",
    "soup = BeautifulSoup(required.text, 'html.parser')\n",
    "\n",
    "articles_info = []\n",
    "for article_div in soup.find_all('div', class_='pod-listing'):\n",
    "    title = article_div.find('a', class_='pod-listing-title').text.strip()\n",
    "    authors = article_div.find('div', class_='text-xs').text.strip()\n",
    "    published_date = article_div.find('div', class_='pod-listing-header').text.strip()\n",
    "    paper_url = article_div.find('a', class_='pod-listing-title')['href']\n",
    "    \n",
    "    articles_info.append({'Paper Title': title, 'Authors': authors, 'Published Date': published_date, 'Paper URL': paper_url})\n",
    "\n",
    "df_articles = pd.DataFrame(articles_info)\n",
    "print(\"Most Downloaded Articles in AI (Last 90 days):\")\n",
    "print(df_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79109002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Details from Dineout:\n",
      "   Restaurant Name Cuisine Location Ratings Image URL\n",
      "0                                                    \n",
      "1                                                    \n",
      "2                                                    \n",
      "3                                                    \n",
      "4                                                    \n",
      "5                                                    \n",
      "6                                                    \n",
      "7                                                    \n",
      "8                                                    \n",
      "9                                                    \n",
      "10                                                   \n",
      "11                                                   \n",
      "12                                                   \n",
      "13                                                   \n",
      "14                                                   \n",
      "15                                                   \n",
      "16                                                   \n",
      "17                                                   \n",
      "18                                                   \n",
      "19                                                   \n",
      "20                                                   \n"
     ]
    }
   ],
   "source": [
    "# QUESTION-7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "required = requests.get(url)\n",
    "soup = BeautifulSoup(required.text, 'html.parser')\n",
    "\n",
    "restaurant_info = []\n",
    "for restaurant_div in soup.find_all('div', class_='restnt-info'):\n",
    "    name_tag = restaurant_div.find('span', class_='restnt-name')\n",
    "    name = name_tag.text.strip() if name_tag else ''\n",
    "\n",
    "    cuisine_tag = restaurant_div.find('span', class_='double-line-ellipsis')\n",
    "    cuisine = cuisine_tag.text.strip() if cuisine_tag else ''\n",
    "\n",
    "    location_tag = restaurant_div.find('span', class_='restnt-Location double-line-ellipsis')\n",
    "    location = location_tag.text.strip() if location_tag else ''\n",
    "\n",
    "    ratings_tag = restaurant_div.find('span', class_='star_score')\n",
    "    ratings = ratings_tag.text.strip() if ratings_tag else ''\n",
    "\n",
    "    image_url_tag = restaurant_div.find('img')\n",
    "    image_url = image_url_tag['data-src'] if image_url_tag else ''\n",
    "\n",
    "    restaurant_info.append({'Restaurant Name': name, 'Cuisine': cuisine, 'Location': location, 'Ratings': ratings, 'Image URL': image_url})\n",
    "\n",
    "df_restaurants = pd.DataFrame(restaurant_info)\n",
    "print(\"Restaurant Details from Dineout:\")\n",
    "print(df_restaurants)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
